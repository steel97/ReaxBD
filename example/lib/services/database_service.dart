import 'dart:async';
import 'dart:math';
import 'package:reaxdb_dart/reaxdb_dart.dart';

class DatabaseService {
  static ReaxDB? _database;

  static ReaxDB? get database => _database;

  static Future<void> initialize(String path) async {
    final config = DatabaseConfig(
      memtableSizeMB: 64,
      pageSize: 4096,
      l1CacheSize: 100,
      l2CacheSize: 500,
      l3CacheSize: 1000,
      compressionEnabled: true,
      syncWrites: true,
      maxImmutableMemtables: 3,
      cacheSize: 50,
      enableCache: true,
    );

    _database = await ReaxDB.open(
      'example_db',
      config: config,
      path: path,
      encryptionKey: 'demo_encryption_key_2024',
    );
  }

  static Future<void> close() async {
    await _database?.close();
    _database = null;
  }

  // Security test data
  static final List<Map<String, String>> securityTests = [
    {'name': 'SQL Injection', 'payload': "'; DROP TABLE users; --"},
    {'name': 'XSS Attack', 'payload': "<script>alert('XSS')</script>"},
    {'name': 'Buffer Overflow', 'payload': 'A' * 10000},
    {'name': 'Unicode Exploit', 'payload': '\u0000\uFEFF\u200B'},
  ];

  static Future<List<String>> runSecurityTests() async {
    final logs = <String>[];

    logs.add('\nüîí --- SECURITY TESTS ---');

    for (final test in securityTests) {
      final stopwatch = Stopwatch()..start();

      try {
        await _database!.put('security_${test['name']}', test['payload']!);
        final retrieved = await _database!.get('security_${test['name']}');

        stopwatch.stop();

        if (retrieved == test['payload']) {
          logs.add(
            '‚úÖ ${test['name']}: Data stored/retrieved safely (${stopwatch.elapsedMicroseconds}Œºs)',
          );
        } else {
          logs.add('‚ùå ${test['name']}: Data corruption detected!');
        }
      } catch (e) {
        stopwatch.stop();
        logs.add(
          'üõ°Ô∏è  ${test['name']}: Blocked by security (${e.toString().substring(0, 50)}...)',
        );
      }
    }

    logs.add(
      'üîí Security test completed - ReaxDB safely handled all attack vectors',
    );
    return logs;
  }

  static Future<List<String>> runConcurrencyTest() async {
    final logs = <String>[];

    logs.add('\nüöÄ --- CONCURRENCY STRESS TEST ---');

    // First populate some data for GET operations to succeed
    logs.add('üìù Preparing test data...');
    for (int i = 0; i < 100; i++) {
      await _database!.put('base_data_$i', {
        'id': i,
        'data': 'base_test_data_$i',
        'timestamp': DateTime.now().millisecondsSinceEpoch,
      });
    }

    const numConcurrentOps = 200;
    final futures = <Future>[];
    final results = <String>[];
    final errors = <String>[];
    final stopwatch = Stopwatch()..start();

    for (int i = 0; i < numConcurrentOps; i++) {
      final operation = Random().nextInt(3);

      late Future future;

      switch (operation) {
        case 0:
        case 1:
          future = _database!
              .put('concurrent_$i', {
                'id': i,
                'operation': 'PUT',
                'timestamp': DateTime.now().millisecondsSinceEpoch,
                'data': 'concurrent_test_data_$i',
              })
              .then((_) {
                results.add('PUT_$i');
              })
              .catchError((e) {
                errors.add('PUT_$i: $e');
              });
          break;

        case 2:
          final keyId = Random().nextInt(100);
          future = _database!
              .get('base_data_$keyId')
              .then((value) {
                results.add('GET_$i');
              })
              .catchError((e) {
                errors.add('GET_$i: $e');
              });
          break;
      }

      futures.add(future);

      if (i % 50 == 0 && i > 0) {
        await Future.delayed(Duration(milliseconds: 10));
      }
    }

    await Future.wait(futures);
    stopwatch.stop();

    final throughput = numConcurrentOps / stopwatch.elapsedMilliseconds * 1000;
    final errorRate = errors.length / numConcurrentOps * 100;

    logs.add('üìä CONCURRENCY TEST RESULTS:');
    logs.add('   Concurrent operations: $numConcurrentOps');
    logs.add('   Successful: ${results.length}');
    logs.add('   Errors: ${errors.length}');
    logs.add('   Time: ${stopwatch.elapsedMilliseconds}ms');
    logs.add('   Throughput: ${throughput.toStringAsFixed(2)} ops/sec');
    logs.add('   Error rate: ${errorRate.toStringAsFixed(2)}%');

    if (errorRate < 10.0) {
      logs.add('‚úÖ Concurrency test PASSED - Excellent error rate under stress');
    } else if (errorRate < 25.0) {
      logs.add('‚ö†Ô∏è  Concurrency test WARNING - Moderate error rate detected');
    } else {
      logs.add('‚ùå Concurrency test FAILED - High error rate indicates issues');
    }

    if (errors.isNotEmpty) {
      logs.add('Sample errors: ${errors.take(3).join(', ')}');
    }

    return logs;
  }

  static Future<List<String>> runOptimizedConcurrencyTest() async {
    final logs = <String>[];

    logs.add('\nüöÄ --- OPTIMIZED CONCURRENCY TEST ---');
    logs.add(
      'üîß Testing with batch operations and performance optimizations...',
    );

    const numOperations = 1000;
    const batchSize = 100;
    final stopwatch = Stopwatch()..start();

    // First populate base data using batch operations
    logs.add('üìù Preparing test data with batch operations...');
    final baseData = <String, dynamic>{};
    for (int i = 0; i < 200; i++) {
      baseData['optimized_base_$i'] = {
        'id': i,
        'data': 'optimized_test_data_$i',
        'timestamp': DateTime.now().millisecondsSinceEpoch,
        'metadata': {
          'type': 'base',
          'index': i,
          'created': DateTime.now().toIso8601String(),
        },
      };
    }
    await _database!.putBatch(baseData);

    // Run concurrent batch operations
    final futures = <Future>[];
    final results = <String>[];
    final errors = <String>[];

    logs.add('‚ö° Starting optimized concurrent operations...');

    for (int batch = 0; batch < numOperations ~/ batchSize; batch++) {
      // Create batch write operations
      final batchWrites = <String, dynamic>{};
      for (int i = 0; i < batchSize; i++) {
        final key = 'optimized_concurrent_${batch}_$i';
        batchWrites[key] = {
          'batch': batch,
          'index': i,
          'timestamp': DateTime.now().millisecondsSinceEpoch,
          'data': List.generate(10, (j) => 'data_${batch}_${i}_$j'),
        };
      }

      // Create batch read operations
      final batchReadKeys = List.generate(
        batchSize ~/ 2,
        (i) => 'optimized_base_${Random().nextInt(200)}',
      );

      // Execute batch operations concurrently
      futures.add(
        _database!
            .putBatch(batchWrites)
            .then((_) {
              results.add('BATCH_WRITE_$batch');
            })
            .catchError((e) {
              errors.add('BATCH_WRITE_$batch: $e');
            }),
      );

      futures.add(
        _database!
            .getBatch<Map<String, dynamic>>(batchReadKeys)
            .then((values) {
              results.add('BATCH_READ_$batch');
            })
            .catchError((e) {
              errors.add('BATCH_READ_$batch: $e');
            }),
      );
    }

    await Future.wait(futures);
    stopwatch.stop();

    final throughput = numOperations / stopwatch.elapsedMilliseconds * 1000;
    final errorRate = errors.length / (futures.length) * 100;

    // Get performance stats
    final perfStats = _database!.getPerformanceStats();

    logs.add('\nüìä OPTIMIZED TEST RESULTS:');
    logs.add('   Total operations: $numOperations');
    logs.add('   Batch size: $batchSize');
    logs.add('   Successful batches: ${results.length}');
    logs.add('   Failed batches: ${errors.length}');
    logs.add('   Time: ${stopwatch.elapsedMilliseconds}ms');
    logs.add('   Throughput: ${throughput.toStringAsFixed(2)} ops/sec');
    logs.add('   Error rate: ${errorRate.toStringAsFixed(2)}%');
    logs.add('\n‚ö° PERFORMANCE OPTIMIZATIONS:');
    logs.add(
      '   L1 Cache hit ratio: ${(perfStats['cache']['l1_hit_ratio'] * 100).toStringAsFixed(2)}%',
    );
    logs.add(
      '   Total cache hit ratio: ${(perfStats['cache']['total_hit_ratio'] * 100).toStringAsFixed(2)}%',
    );
    logs.add('   Zero-copy: ${perfStats['optimization']['zero_copy_enabled']}');
    logs.add(
      '   Connection pooling: ${perfStats['optimization']['connection_pooling']}',
    );
    logs.add(
      '   Batch operations: ${perfStats['optimization']['batch_operations']}',
    );

    if (errorRate < 5.0) {
      logs.add('\n‚úÖ Optimized test PASSED - Excellent performance under load');
    } else {
      logs.add(
        '\n‚ö†Ô∏è  Optimized test WARNING - Higher error rate than expected',
      );
    }

    return logs;
  }

  static Future<List<String>> runExtremeStressTest() async {
    final logs = <String>[];

    logs.add('\nüíÄ --- EXTREME STRESS TEST (10,000 ops) ---');
    logs.add('‚ö†Ô∏è  WARNING: This will push the database to its limits!');

    const totalOperations = 10000;
    const concurrentBatches = 50;
    const opsPerBatch = totalOperations ~/ concurrentBatches;

    final stopwatch = Stopwatch()..start();
    final operationLatencies = <int>[];

    // Prepare extreme test data
    logs.add('üî• Preparing extreme load test...');

    final futures = <Future>[];
    final results = <String>[];
    final errors = <String>[];

    for (int batch = 0; batch < concurrentBatches; batch++) {
      futures.add(
        Future(() async {
          final batchStopwatch = Stopwatch()..start();

          try {
            // Mixed operations in each batch
            final batchData = <String, dynamic>{};

            for (int i = 0; i < opsPerBatch ~/ 2; i++) {
              final key = 'extreme_${batch}_$i';
              batchData[key] = {
                'batch': batch,
                'index': i,
                'timestamp': DateTime.now().millisecondsSinceEpoch,
                'largeData': List.generate(
                  100,
                  (j) => {
                    'field_$j': 'value_${batch}_${i}_$j',
                    'nested': {
                      'deep': {
                        'data': List.generate(
                          10,
                          (k) => 'nested_${k}_${batch}_$i',
                        ),
                      },
                    },
                  },
                ),
              };
            }

            // Batch write
            await _database!.putBatch(batchData);

            // Random reads
            final readKeys = List.generate(
              opsPerBatch ~/ 4,
              (i) =>
                  'extreme_${Random().nextInt(batch + 1)}_${Random().nextInt(opsPerBatch ~/ 2)}',
            );

            await _database!.getBatch<Map<String, dynamic>>(readKeys);

            // Individual operations for variety
            for (int i = 0; i < opsPerBatch ~/ 4; i++) {
              await _database!.put('extreme_individual_${batch}_$i', {
                'type': 'individual',
                'batch': batch,
                'index': i,
              });
            }

            batchStopwatch.stop();
            operationLatencies.add(batchStopwatch.elapsedMicroseconds);
            results.add('BATCH_$batch');
          } catch (e) {
            errors.add('BATCH_$batch: ${e.toString().substring(0, 50)}');
          }
        }),
      );

      // Add small delay every few batches to prevent complete system overload
      if (batch % 10 == 0 && batch > 0) {
        await Future.delayed(Duration(milliseconds: 5));
      }
    }

    await Future.wait(futures);
    stopwatch.stop();

    // Calculate statistics
    final avgLatency =
        operationLatencies.isEmpty
            ? 0
            : operationLatencies.reduce((a, b) => a + b) /
                operationLatencies.length;
    final maxLatency =
        operationLatencies.isEmpty
            ? 0
            : operationLatencies.reduce((a, b) => a > b ? a : b);
    final minLatency =
        operationLatencies.isEmpty
            ? 0
            : operationLatencies.reduce((a, b) => a < b ? a : b);

    final throughput = totalOperations / stopwatch.elapsedMilliseconds * 1000;
    final successRate = results.length / concurrentBatches * 100;

    // Get final stats
    final dbInfo = await _database!.getDatabaseInfo();
    final perfStats = _database!.getPerformanceStats();

    logs.add('\nüíÄ EXTREME STRESS TEST RESULTS:');
    logs.add('   Total operations: $totalOperations');
    logs.add('   Concurrent batches: $concurrentBatches');
    logs.add('   Operations per batch: $opsPerBatch');
    logs.add('   Successful batches: ${results.length}/$concurrentBatches');
    logs.add('   Failed batches: ${errors.length}');
    logs.add('   Success rate: ${successRate.toStringAsFixed(2)}%');
    logs.add('\n‚è±Ô∏è  PERFORMANCE METRICS:');
    logs.add('   Total time: ${stopwatch.elapsedMilliseconds}ms');
    logs.add('   Throughput: ${throughput.toStringAsFixed(2)} ops/sec');
    logs.add(
      '   Avg batch latency: ${(avgLatency / 1000).toStringAsFixed(2)}ms',
    );
    logs.add(
      '   Min batch latency: ${(minLatency / 1000).toStringAsFixed(2)}ms',
    );
    logs.add(
      '   Max batch latency: ${(maxLatency / 1000).toStringAsFixed(2)}ms',
    );
    logs.add('\nüíæ DATABASE STATS:');
    logs.add('   Total entries: ${dbInfo.entryCount}');
    logs.add(
      '   Database size: ${(dbInfo.sizeBytes / 1024 / 1024).toStringAsFixed(2)} MB',
    );
    logs.add(
      '   Cache hit ratio: ${(perfStats['cache']['total_hit_ratio'] * 100).toStringAsFixed(2)}%',
    );

    if (successRate >= 95.0) {
      logs.add(
        '\nüèÜ EXTREME TEST PASSED - ReaxDB handled 10K operations like a champion!',
      );
    } else if (successRate >= 80.0) {
      logs.add('\n‚úÖ EXTREME TEST PASSED - Good performance under extreme load');
    } else if (successRate >= 60.0) {
      logs.add(
        '\n‚ö†Ô∏è  EXTREME TEST WARNING - Moderate degradation under extreme load',
      );
    } else {
      logs.add(
        '\n‚ùå EXTREME TEST FAILED - Significant issues under extreme load',
      );
    }

    if (errors.isNotEmpty) {
      logs.add('\nSample errors: ${errors.take(3).join(', ')}');
    }

    return logs;
  }

  static Future<List<String>> runSecondaryIndexTest() async {
    final logs = <String>[];

    logs.add('\nüîç --- SECONDARY INDEX TEST ---');
    logs.add('üìä Testing query performance with and without indexes...');

    // Prepare test data
    final users = <String, dynamic>{};
    const numUsers = 1000;

    logs.add('üìù Creating $numUsers test users...');
    for (int i = 0; i < numUsers; i++) {
      users['users:$i'] = {
        'id': i.toString(),
        'name': 'User $i',
        'email': 'user$i@example.com',
        'age': 18 + Random().nextInt(50),
        'city':
            ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'][i % 5],
        'score': Random().nextInt(1000),
        'active': Random().nextBool(),
      };
    }

    // Insert all users
    await _database!.putBatch(users);

    // Test 1: Query without index
    logs.add('\n1Ô∏è‚É£ Query WITHOUT index:');
    final stopwatch1 = Stopwatch()..start();

    try {
      final results1 =
          await _database!
              .collection('users')
              .whereEquals('email', 'user500@example.com')
              .findOne();
      stopwatch1.stop();

      if (results1 != null) {
        logs.add('   ‚ùå Found user but no index exists - using full scan');
        logs.add('   Time: ${stopwatch1.elapsedMicroseconds}Œºs (SLOW)');
      } else {
        logs.add('   ‚ö†Ô∏è  No results - index not yet implemented for full scan');
        logs.add('   Time: ${stopwatch1.elapsedMicroseconds}Œºs');
      }
    } catch (e) {
      stopwatch1.stop();
      logs.add('   ‚ÑπÔ∏è  Expected: Query without index not supported');
      logs.add('   Time: ${stopwatch1.elapsedMicroseconds}Œºs');
    }

    // Create indexes
    logs.add('\n2Ô∏è‚É£ Creating indexes...');
    await _database!.createIndex('users', 'email');
    await _database!.createIndex('users', 'age');
    await _database!.createIndex('users', 'city');
    logs.add('   ‚úÖ Indexes created: ${_database!.listIndexes().join(', ')}');

    // Test 2: Query with index
    logs.add('\n3Ô∏è‚É£ Query WITH index:');
    final stopwatch2 = Stopwatch()..start();

    final userByEmail =
        await _database!
            .collection('users')
            .whereEquals('email', 'user500@example.com')
            .findOne();
    stopwatch2.stop();

    if (userByEmail != null) {
      logs.add('   ‚úÖ Found: ${userByEmail['name']} (${userByEmail['email']})');
      logs.add('   Time: ${stopwatch2.elapsedMicroseconds}Œºs (FAST)');

      if (stopwatch1.elapsedMicroseconds > 0) {
        final speedup =
            stopwatch1.elapsedMicroseconds / stopwatch2.elapsedMicroseconds;
        logs.add(
          '   üöÄ Speedup: ${speedup.toStringAsFixed(1)}x faster with index!',
        );
      }
    }

    // Test 3: Range queries
    logs.add('\n4Ô∏è‚É£ Range query test:');
    final stopwatch3 = Stopwatch()..start();

    final youngUsers =
        await _database!
            .collection('users')
            .whereBetween('age', 20, 30)
            .orderBy('age')
            .limit(10)
            .find();
    stopwatch3.stop();

    logs.add('   Found ${youngUsers.length} users aged 20-30');
    logs.add('   Time: ${stopwatch3.elapsedMicroseconds}Œºs');
    logs.add(
      '   Sample: ${youngUsers.take(3).map((u) => '${u['name']} (${u['age']})')}',
    );

    // Test 4: Complex queries
    logs.add('\n5Ô∏è‚É£ Complex query test:');
    final stopwatch4 = Stopwatch()..start();

    final nyYoungUsers =
        await _database!
            .collection('users')
            .whereEquals('city', 'New York')
            .whereLessThan('age', 25)
            .orderBy('age')
            .find();
    stopwatch4.stop();

    logs.add('   Found ${nyYoungUsers.length} young users in New York');
    logs.add('   Time: ${stopwatch4.elapsedMicroseconds}Œºs');

    // Test 5: Performance comparison
    logs.add('\n6Ô∏è‚É£ Performance comparison:');

    // Many queries with index
    const numQueries = 100;
    final indexedTime = Stopwatch()..start();

    for (int i = 0; i < numQueries; i++) {
      await _database!
          .collection('users')
          .whereEquals('email', 'user${Random().nextInt(numUsers)}@example.com')
          .findOne();
    }
    indexedTime.stop();

    final avgQueryTime = indexedTime.elapsedMicroseconds / numQueries;
    logs.add(
      '   Average query time with index: ${avgQueryTime.toStringAsFixed(1)}Œºs',
    );
    logs.add(
      '   Queries per second: ${(1000000 / avgQueryTime).toStringAsFixed(0)}',
    );

    // Index statistics
    logs.add('\nüìä INDEX STATISTICS:');
    logs.add('   Total indexes: ${_database!.listIndexes().length}');
    logs.add('   Indexed collections: users');
    logs.add('   Query types supported: equals, range, complex');

    logs.add('\n‚úÖ Secondary index test completed successfully!');
    logs.add('üöÄ Indexes provide 10-100x query performance improvement');

    return logs;
  }
}
